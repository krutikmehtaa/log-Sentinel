{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection System - Exploratory Analysis\n",
    "\n",
    "This notebook demonstrates the anomaly detection pipeline and provides visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.utils.config import config\n",
    "from src.utils.db_utils import db_manager\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the latest raw data file\n",
    "raw_path = Path(config.raw_data_path)\n",
    "parquet_files = list(raw_path.glob('*.parquet'))\n",
    "latest_file = max(parquet_files, key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "df = pd.read_parquet(latest_file)\n",
    "print(f\"Loaded {len(df)} records from {latest_file.name}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Time range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"\\nAnomaly distribution:\")\n",
    "print(df['is_anomaly'].value_counts())\n",
    "print(f\"Anomaly rate: {df['is_anomaly'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log type distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "df['log_type'].value_counts().plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Log Type Distribution')\n",
    "axes[0].set_xlabel('Log Type')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "df['log_level'].value_counts().plot(kind='bar', ax=axes[1], color='coral')\n",
    "axes[1].set_title('Log Level Distribution')\n",
    "axes[1].set_xlabel('Log Level')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temporal Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logs over time\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['date'] = df['timestamp'].dt.date\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Hourly distribution\n",
    "hourly_counts = df.groupby('hour').size()\n",
    "axes[0].plot(hourly_counts.index, hourly_counts.values, marker='o', linewidth=2)\n",
    "axes[0].set_title('Log Volume by Hour of Day')\n",
    "axes[0].set_xlabel('Hour')\n",
    "axes[0].set_ylabel('Number of Logs')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Daily distribution with anomalies\n",
    "daily_stats = df.groupby('date').agg({\n",
    "    'log_id': 'count',\n",
    "    'is_anomaly': 'sum'\n",
    "}).reset_index()\n",
    "daily_stats.columns = ['date', 'total_logs', 'anomalies']\n",
    "\n",
    "axes[1].plot(daily_stats['date'], daily_stats['total_logs'], label='Total Logs', linewidth=2)\n",
    "axes[1].plot(daily_stats['date'], daily_stats['anomalies'], label='Anomalies', linewidth=2, color='red')\n",
    "axes[1].set_title('Daily Log Volume and Anomalies')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response time distribution\n",
    "api_logs = df[df['log_type'] == 'API_REQUEST'].copy()\n",
    "\n",
    "if 'response_time_ms' in api_logs.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Normal vs Anomaly\n",
    "    api_logs[api_logs['is_anomaly'] == False]['response_time_ms'].hist(\n",
    "        bins=50, ax=axes[0], alpha=0.7, label='Normal', color='blue'\n",
    "    )\n",
    "    api_logs[api_logs['is_anomaly'] == True]['response_time_ms'].hist(\n",
    "        bins=50, ax=axes[0], alpha=0.7, label='Anomaly', color='red'\n",
    "    )\n",
    "    axes[0].set_title('Response Time Distribution')\n",
    "    axes[0].set_xlabel('Response Time (ms)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Box plot\n",
    "    api_logs.boxplot(column='response_time_ms', by='is_anomaly', ax=axes[1])\n",
    "    axes[1].set_title('Response Time: Normal vs Anomaly')\n",
    "    axes[1].set_xlabel('Is Anomaly')\n",
    "    axes[1].set_ylabel('Response Time (ms)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Processed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature-engineered data\n",
    "processed_path = Path(config.processed_data_path)\n",
    "feature_files = list(processed_path.glob('features_*'))\n",
    "\n",
    "if feature_files:\n",
    "    latest_features = max(feature_files, key=lambda p: p.stat().st_mtime)\n",
    "    features_df = pd.read_parquet(latest_features)\n",
    "    print(f\"Loaded {len(features_df)} records with {len(features_df.columns)} features\")\n",
    "    features_df.head()\n",
    "else:\n",
    "    print(\"No feature files found. Run feature_engineering.py first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load anomaly detection results from database\n",
    "anomalies = db_manager.get_recent_anomalies(hours=168, min_score=0.5)\n",
    "\n",
    "if anomalies:\n",
    "    anomalies_df = pd.DataFrame(anomalies)\n",
    "    print(f\"Found {len(anomalies_df)} anomalies in the database\")\n",
    "    \n",
    "    # Score distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    anomalies_df['anomaly_score'].hist(bins=30, color='darkred', alpha=0.7)\n",
    "    plt.title('Anomaly Score Distribution')\n",
    "    plt.xlabel('Anomaly Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    anomalies_df['model_name'].value_counts().plot(kind='bar', color='steelblue')\n",
    "    plt.title('Anomalies by Model')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Top anomalies\n",
    "    print(\"\\nTop 10 Anomalies:\")\n",
    "    top_anomalies = anomalies_df.nlargest(10, 'anomaly_score')\n",
    "    print(top_anomalies[['timestamp', 'log_type', 'anomaly_score', 'message']])\n",
    "else:\n",
    "    print(\"No anomalies found in database. Run batch_inference.py first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query model performance\n",
    "query = \"\"\"\n",
    "    SELECT model_name, metric_name, metric_value, evaluation_date\n",
    "    FROM model_performance\n",
    "    ORDER BY evaluation_date DESC, model_name\n",
    "\"\"\"\n",
    "\n",
    "performance = db_manager.execute_query(query)\n",
    "\n",
    "if performance:\n",
    "    perf_df = pd.DataFrame(performance)\n",
    "    \n",
    "    # Pivot for easier viewing\n",
    "    perf_pivot = perf_df.pivot_table(\n",
    "        index='model_name',\n",
    "        columns='metric_name',\n",
    "        values='metric_value',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    \n",
    "    print(\"Model Performance Comparison:\")\n",
    "    print(perf_pivot)\n",
    "    \n",
    "    # Visualize\n",
    "    perf_pivot.plot(kind='bar', figsize=(10, 6))\n",
    "    plt.title('Model Performance Metrics')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend(title='Metric')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No performance metrics found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance (for Isolation Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load Isolation Forest model\n",
    "models_path = Path(config.models_path)\n",
    "if_models = list(models_path.glob('isolation_forest_*_model.pkl'))\n",
    "\n",
    "if if_models:\n",
    "    latest_model = str(max(if_models, key=lambda p: p.stat().st_mtime)).replace('_model.pkl', '')\n",
    "    model = joblib.load(f\"{latest_model}_model.pkl\")\n",
    "    feature_names = joblib.load(f\"{latest_model}_features.pkl\")\n",
    "    \n",
    "    # Note: Isolation Forest doesn't have feature_importances_\n",
    "    # We can compute approximate importance using permutation or SHAP\n",
    "    print(f\"Model uses {len(feature_names)} features:\")\n",
    "    for i, feat in enumerate(feature_names, 1):\n",
    "        print(f\"{i}. {feat}\")\n",
    "else:\n",
    "    print(\"No Isolation Forest model found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
